{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4624b46b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Charles/nltk_data'\n    - 'C:\\\\Python3\\\\nltk_data'\n    - 'C:\\\\Python3\\\\share\\\\nltk_data'\n    - 'C:\\\\Python3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Charles\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Python3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Charles/nltk_data'\n    - 'C:\\\\Python3\\\\nltk_data'\n    - 'C:\\\\Python3\\\\share\\\\nltk_data'\n    - 'C:\\\\Python3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Charles\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7d7e57a40b6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgutenberg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mStop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mSentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgutenberg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'carroll-alice.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mTermsSentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Charles/nltk_data'\n    - 'C:\\\\Python3\\\\nltk_data'\n    - 'C:\\\\Python3\\\\share\\\\nltk_data'\n    - 'C:\\\\Python3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Charles\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "import re\n",
    "Stop_words = stopwords.words('english')\n",
    "Sentences = gutenberg.sents('carroll-alice.txt')\n",
    "TermsSentences = set({})\n",
    "for terms in Sentences:\n",
    "    terms = [w.lower() for w in terms if w.lower() not in Stop_words]\n",
    "    terms = [w.lower() for w in terms if re.search(r'^[a-zA-Z]{2}', w) is not None]\n",
    "    TermsSentences=TermsSentences.union(set(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "TermsSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49592121",
   "metadata": {},
   "outputs": [],
   "source": [
    "'the' in Stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11295fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TermsSentences = list(TermsSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f878999",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'THE'\" in TermsSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [[] for word in TermsSentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719184c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sentence in Sentences:\n",
    "    for i in range(len(TermsSentences)):\n",
    "        contains = False\n",
    "        for word in sentence:\n",
    "            if TermsSentences[i].lower() == word.lower():\n",
    "                contains = True\n",
    "                break\n",
    "        if contains:\n",
    "            #columns[i].append(1)\n",
    "            columns[i].append('True')\n",
    "        else:\n",
    "            #columns[i].append(np.nan)\n",
    "            columns[i].append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa890200",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array(columns).T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TermsSentences = [ \"'\"+w+\"'\" for w in TermsSentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9430910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns ,columns = TermsSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299899a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"'christmas'\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"'the'\" in list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68734f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./words.csv\",index=False,index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4517e8",
   "metadata": {},
   "source": [
    "Q1\n",
    "Interesting patterns:\n",
    "mock -> turtle with 1 confindence\n",
    "hare -> march with 1 confindence\n",
    "said,turtle -> mock with 1 confidence\n",
    "said,hare -> march with 1 confidence\n",
    "join -> dance with 1 confidence\n",
    "glass -> little with .9 confidence\n",
    "white -> rabbit with .83 confidence\n",
    "beautiful -> soup with .82 confidence\n",
    "afraid -> alice with .83 confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad0158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP(object):\n",
    "\n",
    "    def __init__(self, n_hidden=30, epochs=100, eta=0.001, minibatch_size=1, seed=None):\n",
    "        self.random = np.random.RandomState(seed)  # used to randomize weights\n",
    "        self.n_hidden = n_hidden  # size of the hidden layer\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = minibatch_size  # size of training batch - 1 would not work\n",
    "        self.w_out, self.w_h, self.w_h_2 = None, None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def onehot(_y, _n_classes):  # one hot encode the input class y\n",
    "        onehot = np.zeros((_n_classes, _y.shape[0]))\n",
    "        for idx, val in enumerate(_y.astype(int)):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot.T\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(_z):  # Eq 1\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(_z, -250, 250)))\n",
    "\n",
    "    def _forward(self, _X):  # Eq 2\n",
    "        z_h = np.dot(_X, self.w_h)\n",
    "        a_h = self.sigmoid(z_h)\n",
    "\n",
    "        #\n",
    "        z_h_2 = np.dot(a_h, self.w_h_2)\n",
    "        a_h_2 = self.sigmoid(z_h_2)\n",
    "        #\n",
    "        z_out = np.dot(a_h_2, self.w_out)\n",
    "        a_out = self.sigmoid(z_out)\n",
    "        return z_h, a_h, z_out, a_out, z_h_2, a_h_2\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cost(y_enc, output):  # Eq 4\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, _X):\n",
    "        z_h, a_h, z_out, a_out, z_h_2, a_h_2 = self._forward(_X)\n",
    "        ypred = np.argmax(z_out, axis=1)\n",
    "        return ypred\n",
    "\n",
    "    def fit(self, _X_train, _y_train, _X_valid, _y_valid):\n",
    "        import sys\n",
    "        n_output = np.unique(_y_train).shape[0]  # number of class labels\n",
    "        n_features = _X_train.shape[1]\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
    "        self.w_h = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden))\n",
    "        #\n",
    "        self.w_h_2 = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, self.n_hidden))\n",
    "        #\n",
    "        y_train_enc = self.onehot(_y_train, n_output)  # one-hot encode original y\n",
    "        for ei in range(self.epochs):  # Ideally must shuffle at every epoch\n",
    "            indices = np.arange(_X_train.shape[0])\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "\n",
    "                z_h, a_h, z_out, a_out, z_h_2, a_h_2 = self._forward(_X_train[batch_idx])  # neural network model\n",
    "\n",
    "                sigmoid_derivative_h = a_h * (1.0 - a_h)  # Eq 3\n",
    "                #\n",
    "                sigmoid_derivative_h_2 = a_h_2 * (1.0 - a_h_2)\n",
    "                #\n",
    "                delta_out = a_out - y_train_enc[batch_idx]  # Eq 5\n",
    "                delta_h_2 = (np.dot(delta_out, self.w_out.T) * sigmoid_derivative_h_2)  # Eq 6\n",
    "                #\n",
    "                delta_h = (np.dot(delta_h_2, self.w_h_2.T) * sigmoid_derivative_h)\n",
    "                #\n",
    "                grad_w_out = np.dot(a_h_2.T, delta_out)  # Eq 7\n",
    "                grad_w_h_2 = np.dot(a_h.T, delta_h_2)  # Eq 8\n",
    "\n",
    "                grad_w_h = np.dot(_X_train[batch_idx].T, delta_h)\n",
    "\n",
    "                self.w_out -= self.eta * grad_w_out  # Eq 9\n",
    "\n",
    "                self.w_h_2 -= self.eta * grad_w_h_2\n",
    "\n",
    "                self.w_h -= self.eta * grad_w_h  # Eq 9\n",
    "\n",
    "            # Evaluation after each epoch during training\n",
    "            z_h, a_h, z_out, a_out, z_h_2, a_h_2 = self._forward(_X_train)\n",
    "            cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            y_train_pred = self.predict(_X_train)  # monitoring training progress through reclassification\n",
    "            y_valid_pred = self.predict(_X_valid)  # monitoring training progress through validation\n",
    "            train_acc = ((np.sum(_y_train == y_train_pred)).astype(float) / _X_train.shape[0])\n",
    "            valid_acc = ((np.sum(_y_valid == y_valid_pred)).astype(float) / _X_valid.shape[0])\n",
    "            sys.stderr.write('\\r%d/%d | Cost: %.2f ' '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
    "                             (ei + 1, self.epochs, cost, train_acc * 100, valid_acc * 100))\n",
    "            sys.stderr.flush()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    from numpy import fromfile, uint8\n",
    "    import os\n",
    "    import struct\n",
    "    \n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = fromfile(lbpath, dtype=uint8)\n",
    "        with open(images_path, 'rb') as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "            images = fromfile(imgpath, dtype=uint8).reshape(len(labels), 784)\n",
    "            images = ((images / 255.) - .5) * 2\n",
    "    return images, labels\n",
    "\n",
    "X_train_mnist, y_train_mnist = load_mnist('./EP_datasets/mnist/', kind='train')\n",
    "print(f'Rows= {X_train_mnist.shape[0]}, columns= {X_train_mnist.shape[1]}')\n",
    "\n",
    "X_test_mnist, y_test_mnist = load_mnist('./EP_datasets/mnist/', kind='t10k')\n",
    "print(f'Rows= {X_test_mnist.shape[0]}, columns= {X_test_mnist.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937af077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'Solver terminated early.*')\n",
    "\n",
    "def get_acc(_y_test, _y_pred):\n",
    "    return (np.sum(_y_test == _y_pred)).astype(float) / _y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44773187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define and fit the neural network\n",
    "nn = NeuralNetMLP(n_hidden=20, epochs=300, eta=0.0005, minibatch_size=100, seed=1)\n",
    "\n",
    "nn.fit(X_train_mnist[:55000], y_train_mnist[:55000], X_train_mnist[55000:], y_train_mnist[55000:]) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test_mnist)\n",
    "\n",
    "print(f'Accuracy= {get_acc(y_test_mnist, y_pred)*100:.2f}%')\n",
    "print(confusion_matrix(y_test_mnist, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe33105",
   "metadata": {},
   "source": [
    "Q1:\n",
    "The environment is limited, deterministic, and the states are completely observable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de265280",
   "metadata": {},
   "source": [
    "Q2:\n",
    "The agents are:\n",
    "qlearner - performs actions based on the qlearning equation\n",
    "guru - performs the best action mathematically\n",
    "random - makes actions randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d8b07",
   "metadata": {},
   "source": [
    "Q3:\n",
    "The reward is a positive number that is used to update the learning table in case the learner wins. The penalty is negative and is for losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8a3e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "[218]\n"
     ]
    }
   ],
   "source": [
    "states = set({})\n",
    "def calculate_states(current,total_actions,add_one):\n",
    "    if current in states:\n",
    "        return\n",
    "    states.add(current)\n",
    "    if add_one:\n",
    "        total_actions[0]+=1\n",
    "    for i in range(3):\n",
    "        for j in range(1,current[0][i]):\n",
    "            n = list(current[0])\n",
    "            n[i]-=j\n",
    "            n = sorted(n)#\n",
    "            calculate_states((tuple(n),1-current[1]),total_actions, not add_one)\n",
    "\n",
    "total_actions = [0]\n",
    "calculate_states( ((10,10,10),0),total_actions, False )\n",
    "print(len(states))\n",
    "print(total_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51dbe97",
   "metadata": {},
   "source": [
    "Q4:11^3 is 1,331."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c60c3",
   "metadata": {},
   "source": [
    "Q5:Sum of the first 30 numbers is 465."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82780d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game()->list:\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(_st:list)->(int,int):\n",
    "    xored = _st[0] ^ _st[1] ^ _st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(_st)\n",
    "    for pile in range(3):\n",
    "        s = _st[pile] ^ xored\n",
    "        if s <= _st[pile]:\n",
    "            return _st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st:list)->(int,int):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4f5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st:list)->(int,int):\n",
    "    global qtable\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181c9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(_a:str, _b:str):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[_a] if side == 'A' else Engines[_b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n:int, _a:str, _b:str)->(int,int):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for _ in range(_n):\n",
    "        wins[game(_a, _b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {_a:>8s}{wins['A']:5d}  {_b:>8s}{wins['B']:5d}\")\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1e2b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,   Random  505    Random  495\n",
      "1000 games,     Guru 1000    Random    0\n",
      "1000 games,   Random    6      Guru  994\n",
      "1000 games,     Guru  949      Guru   51\n"
     ]
    }
   ],
   "source": [
    "play_games(1000, 'Random', 'Random')\n",
    "play_games(1000, 'Guru', 'Random')\n",
    "play_games(1000, 'Random', 'Guru')\n",
    "play_games(1000, 'Guru', 'Guru') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86148736",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1, 0.8, 100.0#change alpha from 1 to #change gamma from .8 to\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n:int):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=np.float32)\n",
    "    # play _n games\n",
    "    for _ in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "\n",
    "            qtable_update(-Reward, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            #penalize losses ^\n",
    "            # Switch sides for play and learning\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r:float, _st1:list, move:int, pile:int, q_future_best:float):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7eeb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nim_qlearn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2804b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  762    Random  238\n",
      "1000 games,   Random  241  Qlearner  759\n",
      "1000 games,   Random  479    Random  521\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Random', 'Qlearner')\n",
    "\n",
    "play_games(1000, 'Random', 'Random') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a91500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  541    Random  459\n",
      "1000 games, Qlearner  593    Random  407\n",
      "1000 games, Qlearner  689    Random  311\n",
      "1000 games, Qlearner  771    Random  229\n",
      "1000 games, Qlearner  784    Random  216\n",
      "1000 games, Qlearner  775    Random  225\n",
      "1000 games, Qlearner  782    Random  218\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "Wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    wins_a, wins_b = play_games(1000, 'Qlearner', 'Random')\n",
    "    Wins += [wins_a/(wins_a+wins_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf569f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.541, 0.593, 0.689, 0.771, 0.784, 0.775, 0.782]\n"
     ]
    }
   ],
   "source": [
    "# Check the ratio of wins wrt to size of the reinforcement model training\n",
    "print(Wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f296f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn:str):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            print(s, file=fout)\n",
    "\n",
    "qtable_log('qtable_debug.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a75ebc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner    5      Guru  995\n",
      "1000 games, Qlearner    2      Guru  998\n",
      "1000 games, Qlearner    3      Guru  997\n",
      "1000 games, Qlearner   10      Guru  990\n",
      "1000 games, Qlearner   16      Guru  984\n",
      "1000 games, Qlearner   11      Guru  989\n",
      "1000 games, Qlearner   20      Guru  980\n",
      "[0.005, 0.002, 0.003, 0.01, 0.016, 0.011, 0.02]\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "Wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    wins_a, wins_b = play_games(1000, 'Qlearner', 'Guru')\n",
    "    Wins += [wins_a/(wins_a+wins_b)]\n",
    "# Check the ratio of wins wrt to size of the reinforcement model training\n",
    "print(Wins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a741353",
   "metadata": {},
   "source": [
    "Q6:\n",
    "By simply changing the penalty for a loss from 0 to `-Reward`, we get improved performance in the QLearner.  I'm not sure how to beat the Guru."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

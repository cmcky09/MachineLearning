{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0a3b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\phill\\anaconda3\\envs\\tf\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\phill\\anaconda3\\envs\\tf\\lib\\site-packages (from opencv-python) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e33f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c920da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMGSIZE = (128, 128)\n",
    "CNAMES = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "X_tr, y_tr, X_ts, y_ts = [], [], [], []\n",
    "_path = '.'\n",
    "for label in CNAMES:\n",
    "    path = _path + '/seg_train/seg_train/' + label\n",
    "    for f in sorted([_ for _ in os.listdir(path) if _.lower().endswith('.jpg')]):\n",
    "        X_tr += [cv2.resize(cv2.imread(os.path.join(path,f)), IMGSIZE)]\n",
    "        y_tr += [CNAMES.index(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f88a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGSIZE = (128, 128)\n",
    "CNAMES = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
    "for label in CNAMES:\n",
    "    path = \".\" + '/seg_test/seg_test/' + label\n",
    "    for f in sorted([_ for _ in os.listdir(path) if _.lower().endswith('.jpg')]):\n",
    "        X_ts += [cv2.resize(cv2.imread(os.path.join(path,f)), IMGSIZE)]\n",
    "        y_ts += [CNAMES.index(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70e569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14233e1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:1261: error: (-27:Null pointer) NULL window: 'i' in function 'cvDestroyWindow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m,np\u001b[38;5;241m.\u001b[39marray(X_tr[\u001b[38;5;241m0\u001b[39m] ,dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8))\n\u001b[0;32m      2\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyWindow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window_w32.cpp:1261: error: (-27:Null pointer) NULL window: 'i' in function 'cvDestroyWindow'\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow('i',np.array(X_tr[0] ,dtype=np.uint8))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ce68c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "number_of_color_channels=X_tr[0].shape[2]\n",
    "number_of_color_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f077a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14034, 128, 128, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "X_trnp = np.array(X_tr)\n",
    "y_trnp = np.array(y_tr)\n",
    "X_tsnp = np.array(X_ts)\n",
    "y_tsnp = np.array(y_ts)\n",
    "X_trnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5facdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trscaled = X_trnp/255\n",
    "X_tsscaled = X_tsnp/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05305e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb51cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96078431, 0.96862745, 0.96862745],\n",
       "       [0.96078431, 0.96862745, 0.96862745],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96470588, 0.97647059, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96470588, 0.97647059, 0.96862745],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.97254902, 0.98431373, 0.97647059],\n",
       "       [0.97254902, 0.98823529, 0.96862745],\n",
       "       [0.97254902, 0.98823529, 0.96862745],\n",
       "       [0.96862745, 0.98431373, 0.96470588],\n",
       "       [0.96470588, 0.98039216, 0.96470588],\n",
       "       [0.95686275, 0.97647059, 0.96862745],\n",
       "       [0.95686275, 0.97647059, 0.96862745],\n",
       "       [0.95686275, 0.97647059, 0.96862745],\n",
       "       [0.95294118, 0.97254902, 0.96470588],\n",
       "       [0.95294118, 0.97254902, 0.96470588],\n",
       "       [0.95294118, 0.97254902, 0.96470588],\n",
       "       [0.96078431, 0.98039216, 0.97254902],\n",
       "       [0.96078431, 0.98039216, 0.97254902],\n",
       "       [0.96078431, 0.98039216, 0.97254902],\n",
       "       [0.96078431, 0.98039216, 0.97254902],\n",
       "       [0.96470588, 0.98431373, 0.97647059],\n",
       "       [0.96470588, 0.98431373, 0.97647059],\n",
       "       [0.96078431, 0.98039216, 0.97254902],\n",
       "       [0.96470588, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96470588, 0.97647059, 0.96862745],\n",
       "       [0.96078431, 0.97254902, 0.96470588],\n",
       "       [0.96862745, 0.97254902, 0.96470588],\n",
       "       [0.97647059, 0.98039216, 0.97254902],\n",
       "       [0.97254902, 0.97647059, 0.96862745],\n",
       "       [0.97254902, 0.97647059, 0.96862745],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.98039216, 0.97254902],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96078431, 0.96862745, 0.96862745],\n",
       "       [0.96078431, 0.96862745, 0.96862745],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97647059, 0.98431373, 0.98431373],\n",
       "       [0.97254902, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97647059, 0.98431373, 0.98431373],\n",
       "       [0.97647059, 0.98431373, 0.98431373],\n",
       "       [0.97254902, 0.98039216, 0.98431373],\n",
       "       [0.97254902, 0.98039216, 0.98431373],\n",
       "       [0.97254902, 0.98039216, 0.98431373],\n",
       "       [0.98039216, 0.97647059, 0.98431373],\n",
       "       [0.97647059, 0.98039216, 0.98431373],\n",
       "       [0.97647059, 0.98431373, 0.98431373],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.97647059, 0.98431373, 0.98431373],\n",
       "       [0.98039216, 0.98823529, 0.98823529],\n",
       "       [0.98431373, 0.98431373, 0.98431373],\n",
       "       [0.98039216, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.98039216, 0.98039216],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96470588, 0.97254902, 0.97254902],\n",
       "       [0.96078431, 0.97647059, 0.97254902],\n",
       "       [0.95686275, 0.97647059, 0.97254902],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.96862745, 0.97647059, 0.97647059],\n",
       "       [0.97254902, 0.97647059, 0.97647059],\n",
       "       [0.97647059, 0.97647059, 0.97647059],\n",
       "       [0.97647059, 0.97647059, 0.97647059],\n",
       "       [0.97647059, 0.97647059, 0.97647059],\n",
       "       [0.98039216, 0.98039216, 0.98039216],\n",
       "       [0.97254902, 0.97254902, 0.97254902],\n",
       "       [0.97254902, 0.97254902, 0.97254902],\n",
       "       [0.96470588, 0.96862745, 0.96862745],\n",
       "       [0.96078431, 0.96862745, 0.96862745]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trscaled[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e472a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d9669d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr = np.array(y_tr)\n",
    "y_ts = np.array(y_ts)\n",
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3e3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c0464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# Our full CNN neural network\n",
    "cnn1 = tf.keras.Sequential()\n",
    "\n",
    "# if necessary keep the image size same padding='same'\n",
    "cnn1.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(5, 5),\n",
    "    data_format='channels_last',\n",
    "    name='conv_1', activation='relu'))\n",
    "\n",
    "cnn1.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_1'))  # 12x12 image size\n",
    "\n",
    "# if necessary keep the image size same padding='same'\n",
    "cnn1.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5),\n",
    "    name='conv_2', activation='relu'))\n",
    "\n",
    "cnn1.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_2'))  # 4x4 image size\n",
    "\n",
    "# parameter size computed from the previous image sizes and pooling layers\n",
    "cnn1.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn1.add(tf.keras.layers.Dense(units=256, name='fc_1', activation='relu'))\n",
    "\n",
    "cnn1.add(tf.keras.layers.Dense(units=10, name='fc_2', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27eac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for repeatibility\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Build the model\n",
    "cnn1.build(input_shape=(None, 128, 128, 3))\n",
    "\n",
    "# Compile the model with the optimizer, loss function and metric\n",
    "cnn1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "NUM_EPOCHS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b34bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights for debugging purposes and saving the model\n",
    "cnn1.save_weights('cnn1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66c32355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "439/439 [==============================] - 195s 432ms/step - loss: 0.9812 - accuracy: 0.6283\n",
      "Epoch 2/7\n",
      "439/439 [==============================] - 182s 415ms/step - loss: 0.6853 - accuracy: 0.7479\n",
      "Epoch 3/7\n",
      "439/439 [==============================] - 180s 411ms/step - loss: 0.4856 - accuracy: 0.8234\n",
      "Epoch 4/7\n",
      "439/439 [==============================] - 188s 428ms/step - loss: 0.2938 - accuracy: 0.8938\n",
      "Epoch 5/7\n",
      "439/439 [==============================] - 181s 412ms/step - loss: 0.1716 - accuracy: 0.9427\n",
      "Epoch 6/7\n",
      "439/439 [==============================] - 181s 413ms/step - loss: 0.1173 - accuracy: 0.9617\n",
      "Epoch 7/7\n",
      "439/439 [==============================] - 180s 411ms/step - loss: 0.0904 - accuracy: 0.9738\n",
      "CPU times: total: 46min 30s\n",
      "Wall time: 23min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = cnn1.fit(X_trscaled, y_tr,\n",
    "                   epochs=NUM_EPOCHS,\n",
    "                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "675b7ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 11s 112ms/step\n",
      "Accuracy= 0.754\n"
     ]
    }
   ],
   "source": [
    "# Testing dataset\n",
    "y_pred = np.argmax(cnn1.predict(X_tsscaled), axis=1)\n",
    "print(f'Accuracy= {sum(y_pred==y_ts)/len(X_ts):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54b2180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 46s 103ms/step\n",
      "Accuracy= 0.988\n"
     ]
    }
   ],
   "source": [
    "y_reclass = np.argmax(cnn1.predict(X_trscaled), axis=1)\n",
    "print(f'Accuracy= {sum(y_reclass==y_tr)/len(X_tr):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7d6f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c5ae812",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# Our full CNN neural network\n",
    "cnn2 = tf.keras.Sequential()\n",
    "\n",
    "# if necessary keep the image size same padding='same'\n",
    "cnn2.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(5, 5),\n",
    "    data_format='channels_last',\n",
    "    name='conv_1', activation='relu'))\n",
    "\n",
    "cnn2.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_1'))  # 12x12 image size\n",
    "\n",
    "# if necessary keep the image size same padding='same'\n",
    "cnn2.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5),\n",
    "    name='conv_2', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.1)))\n",
    "\n",
    "cnn2.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='pool_2'))  # 4x4 image size\n",
    "\n",
    "# parameter size computed from the previous image sizes and pooling layers\n",
    "cnn2.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn2.add(tf.keras.layers.Dense(units=256, name='fc_1', activation='relu'))\n",
    "\n",
    "cnn2.add(tf.keras.layers.Dense(units=10, name='fc_2', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2059fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for repeatibility\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Build the model\n",
    "cnn2.build(input_shape=(None, 128, 128, 3))\n",
    "\n",
    "# Compile the model with the optimizer, loss function and metric\n",
    "cnn2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baba9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights for debugging purposes and saving the model\n",
    "cnn2.save_weights('cnn2_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "547abebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "439/439 [==============================] - 195s 427ms/step - loss: 1.4717 - accuracy: 0.6223\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 194s 443ms/step - loss: 0.8570 - accuracy: 0.7085\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 180s 409ms/step - loss: 0.7120 - accuracy: 0.7639\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 179s 407ms/step - loss: 0.6282 - accuracy: 0.7927\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 182s 415ms/step - loss: 0.5568 - accuracy: 0.8192\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 196s 447ms/step - loss: 0.4685 - accuracy: 0.8573\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 194s 443ms/step - loss: 0.3689 - accuracy: 0.8961\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 195s 444ms/step - loss: 0.3268 - accuracy: 0.9119\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 182s 414ms/step - loss: 0.2616 - accuracy: 0.9367\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 186s 424ms/step - loss: 0.2412 - accuracy: 0.9424\n",
      "CPU times: total: 1h 10min 51s\n",
      "Wall time: 32min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = cnn2.fit(X_trscaled, y_tr,\n",
    "                   epochs=NUM_EPOCHS,\n",
    "                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e40b47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 10s 106ms/step\n",
      "Accuracy= 0.709\n"
     ]
    }
   ],
   "source": [
    "# Testing dataset\n",
    "y_pred = np.argmax(cnn2.predict(X_tsscaled), axis=1)\n",
    "print(f'Accuracy= {sum(y_pred==y_ts)/len(X_ts):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03d86e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 47s 107ms/step\n",
      "Accuracy= 0.950\n"
     ]
    }
   ],
   "source": [
    "y_reclass = np.argmax(cnn2.predict(X_trscaled), axis=1)\n",
    "print(f'Accuracy= {sum(y_reclass==y_tr)/len(X_tr):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e7052",
   "metadata": {},
   "source": [
    "q4:\n",
    "A model that overfits the data will have higher standard deviation on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
